{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n",
      "Model on CUDA? True\n"
     ]
    }
   ],
   "source": [
    "from experiment import *\n",
    "from experiment import BasicCNN\n",
    "\n",
    "\n",
    "# Setup: initialize the hyperparameters/variables\n",
    "num_epochs = 1           # Number of full passes through the dataset\n",
    "batch_size = 16          # Number of samples in each minibatch\n",
    "learning_rate = 0.001  \n",
    "seed = np.random.seed(1) # Seed the random number generator for reproducibility\n",
    "p_val = 0.1              # Percent of the overall dataset to reserve for validation\n",
    "p_test = 0.2             # Percent of the overall dataset to reserve for testing\n",
    "\n",
    "#TODO: Convert to Tensor - you can later add other transformations, such as Scaling here\n",
    "transform = transforms.Compose([transforms.Resize((512,512)), transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "# Setup the training, validation, and testing dataloaders\n",
    "train_loader, val_loader, test_loader = create_split_loaders(batch_size, seed, transform=transform, \n",
    "                                                             p_val=p_val, p_test=p_test,\n",
    "                                                             shuffle=True, show_sample=False, \n",
    "                                                             extras=extras)\n",
    "\n",
    "# Instantiate a BasicCNN to run on the GPU or CPU based on CUDA support\n",
    "model = BasicCNN()\n",
    "model = model.to(computing_device)\n",
    "print(\"Model on CUDA?\", next(model.parameters()).is_cuda)\n",
    "\n",
    "#TODO: Define the loss criterion and instantiate the gradient descent optimizer\n",
    "criterion = nn.BCELoss() #TODO - loss criteria are defined in the torch.nn package\n",
    "\n",
    "#TODO: Instantiate the gradient descent optimizer - use Adam optimizer with default parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001) #TODO - optimizers are defined in the torch.optim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, average minibatch 0 loss: 0.015\n",
      "Epoch 1, average minibatch 50 loss: 0.729\n",
      "Epoch 1, average minibatch 100 loss: 0.721\n",
      "Epoch 1, average minibatch 150 loss: 0.715\n",
      "Epoch 1, average minibatch 200 loss: 0.713\n",
      "Epoch 1, average minibatch 250 loss: 0.712\n",
      "Epoch 1, average minibatch 300 loss: 0.707\n",
      "Epoch 1, average minibatch 350 loss: 0.705\n",
      "Epoch 1, average minibatch 400 loss: 0.702\n",
      "Epoch 1, average minibatch 450 loss: 0.699\n",
      "Epoch 1, average minibatch 500 loss: 0.694\n",
      "Epoch 1, average minibatch 550 loss: 0.693\n",
      "Epoch 1, average minibatch 600 loss: 0.690\n",
      "Epoch 1, average minibatch 650 loss: 0.691\n",
      "Epoch 1, average minibatch 700 loss: 0.685\n",
      "Epoch 1, average minibatch 750 loss: 0.684\n",
      "Epoch 1, average minibatch 800 loss: 0.681\n",
      "Epoch 1, average minibatch 850 loss: 0.678\n",
      "Epoch 1, average minibatch 900 loss: 0.678\n",
      "Epoch 1, average minibatch 950 loss: 0.673\n",
      "Epoch 1, average minibatch 1000 loss: 0.669\n",
      "Epoch 1, average minibatch 1050 loss: 0.670\n",
      "Epoch 1, average minibatch 1100 loss: 0.666\n",
      "Epoch 1, average minibatch 1150 loss: 0.663\n",
      "Epoch 1, average minibatch 1200 loss: 0.660\n",
      "Epoch 1, average minibatch 1250 loss: 0.657\n",
      "Epoch 1, average minibatch 1300 loss: 0.657\n",
      "Epoch 1, average minibatch 1350 loss: 0.655\n",
      "Epoch 1, average minibatch 1400 loss: 0.651\n",
      "Epoch 1, average minibatch 1450 loss: 0.650\n",
      "Epoch 1, average minibatch 1500 loss: 0.647\n",
      "Epoch 1, average minibatch 1550 loss: 0.645\n",
      "Epoch 1, average minibatch 1600 loss: 0.642\n",
      "Epoch 1, average minibatch 1650 loss: 0.641\n",
      "Epoch 1, average minibatch 1700 loss: 0.638\n",
      "Epoch 1, average minibatch 1750 loss: 0.636\n",
      "Epoch 1, average minibatch 1800 loss: 0.633\n",
      "Epoch 1, average minibatch 1850 loss: 0.631\n",
      "Epoch 1, average minibatch 1900 loss: 0.630\n",
      "Epoch 1, average minibatch 1950 loss: 0.627\n",
      "Epoch 1, average minibatch 2000 loss: 0.624\n",
      "Epoch 1, average minibatch 2050 loss: 0.622\n",
      "Epoch 1, average minibatch 2100 loss: 0.623\n",
      "Epoch 1, average minibatch 2150 loss: 0.618\n",
      "Epoch 1, average minibatch 2200 loss: 0.615\n",
      "Epoch 1, average minibatch 2250 loss: 0.613\n",
      "Epoch 1, average minibatch 2300 loss: 0.612\n",
      "Epoch 1, average minibatch 2350 loss: 0.610\n",
      "Epoch 1, average minibatch 2400 loss: 0.608\n",
      "Epoch 1, average minibatch 2450 loss: 0.605\n",
      "Epoch 1, average minibatch 2500 loss: 0.604\n",
      "Epoch 1, average minibatch 2550 loss: 0.600\n",
      "Epoch 1, average minibatch 2600 loss: 0.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-035784739463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Add this iteration's loss to the total_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mN_minibatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Track the loss across training\n",
    "total_loss = []\n",
    "avg_minibatch_loss = []\n",
    "\n",
    "# Begin training procedure\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    N = 50\n",
    "    N_minibatch_loss = 0.0    \n",
    "\n",
    "    # Get the next minibatch of images, labels for training\n",
    "    for minibatch_count, (images, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "        labels1=torch.tensor(labels, dtype=torch.long, device=computing_device)\n",
    "\n",
    "        # Zero out the stored gradient (buffer) from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs = model(images)\n",
    "        v_o,i_o=torch.max(outputs,1)\n",
    "        v_l, i_l=torch.max(labels1,1)\n",
    "        loss = criterion(outputs,labels)\n",
    "        \n",
    "        # Automagically compute the gradients and backpropagate the loss through the network\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add this iteration's loss to the total_loss\n",
    "        total_loss.append(loss.item())\n",
    "        N_minibatch_loss += loss\n",
    "        \n",
    "        #TODO: Implement cross-validation\n",
    "        \n",
    "        if minibatch_count % N == 0:    \n",
    "            \n",
    "            # Print the loss averaged over the last N mini-batches    \n",
    "            N_minibatch_loss /= N\n",
    "            print('Epoch %d, average minibatch %d loss: %.3f' %\n",
    "                (epoch + 1, minibatch_count, N_minibatch_loss))\n",
    "            \n",
    "            # Add the averaged loss over N minibatches and reset the counter\n",
    "            avg_minibatch_loss.append(N_minibatch_loss)\n",
    "            N_minibatch_loss = 0.0\n",
    "\n",
    "    print(\"Finished\", epoch + 1, \"epochs of training\")\n",
    "print(\"Training complete after\", epoch, \"epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    o=(outputs>0.5).float()\n",
    "    return torch.sum(o==labels,0).float()/labels.shape[0]\n",
    "\n",
    "def true_positive(outputs, labels):\n",
    "    o=(outputs>0.5).float()\n",
    "    return torch.sum(o+labels ==2.,0)\n",
    "\n",
    "def false_positive(outputs, labels):\n",
    "    o=(outputs>0.5).float()\n",
    "    return torch.sum((labels-o)<0,0)\n",
    "\n",
    "def false_negative(outputs, labels):\n",
    "    o=(outputs>0.5).float()\n",
    "    return torch.sum((o-labels)<0,0)\n",
    "\n",
    "def precision(outputs, labels):\n",
    "    return (true_positive(outputs, labels))/(true_positive(outputs, labels)+false_positive(outputs, labels))\n",
    "\n",
    "def recall(outputs, labels):\n",
    "    return (true_positive(outputs, labels))/(true_positive(outputs, labels)+false_negative(outputs, labels))\n",
    "\n",
    "def BCR(outputs, labels):\n",
    "    return (precision(outputs, labels)+recall(outputs, labels))/2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
